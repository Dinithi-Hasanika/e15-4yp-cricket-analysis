# -*- coding: utf-8 -*-
"""Bowler_Score_Econ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sIUqWM7lSNuSN7Ko77s_VP5r5922ge4B
"""

from google.colab import auth
auth.authenticate_user()
import gspread
from oauth2client.client import GoogleCredentials
gc = gspread.authorize(GoogleCredentials.get_application_default())

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

wb = gc.open_by_url('https://docs.google.com/spreadsheets/d/1peLpNFoEu8NHtVPttJXz4-aScixYu5dh_-6gYPOi9rQ/edit#gid=781939969')

sheet = wb.worksheet('bo_data_clean')

data = sheet.get_all_values()

df = pd.DataFrame(data)
df.columns = df.iloc[0]
df = df.iloc[1:]

df.head()

df.dtypes

df["Height (cm)"]= df["Height (cm)"].astype(float)
df["Man of the match"]= df["Man of the match"].astype(int)
df["Inns"]= df["Inns"].astype(int)
df["Mat"]= df["Mat"].astype(int)
df["Balls"]= df["Balls"].astype(int)
df["Runs"]= df["Runs"].astype(int)
df["Wkts"]= df["Wkts"].astype(int)
df["BBI-calc"]= df["BBI-calc"].astype(float)
df["Ave"]= df["Ave"].astype(float)
df["Econ"]= df["Econ"].astype(float)
df["SR"]= df["SR"].astype(float)
df["4"]= df["4"].astype(int)
df["5"]= df["5"].astype(int)
df["Match_Win_Ave"]= df["Match_Win_Ave"].astype(float)
df["Hat tricks"]= df["Hat tricks"].astype(int)

df.dtypes

from sklearn.preprocessing import LabelEncoder
#Since python machine learning algorithm do not accept string values
le = LabelEncoder()
df['Bowling Style'] = le.fit_transform(df['Bowling Style'])

df.head()

"""## **With Career Features**"""

feature_names = ['Height (cm)', 'Man of the match', 'Bowling Style', 'Runs', 'Wkts', 'Ave', 'Match_Win_Ave', 'SR', '4', '5', 'Hat tricks', 'BBI-calc']
X = df[feature_names]
Y = df["Econ"]

#split the data set as training set and test set randomly
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)

#apply scaling
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""**Linear Regression Feature importance**"""

from sklearn.linear_model import LinearRegression
from matplotlib import pyplot

linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)

lin_importance = linear_reg.coef_
for i,v in enumerate(lin_importance):
	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names[i],v))
 
# plot feature importance
pyplot.bar([x for x in range(len(lin_importance))], lin_importance)
pyplot.show()

print('Accuracy of Linear regressor on training set: {:.2f}'
     .format(linear_reg.score(X_train, y_train)))
print('Accuracy of Linear regressor on test set: {:.2f}'
     .format(linear_reg.score(X_test, y_test)))

"""**CART Regression Feature Importance**"""

from sklearn.tree import DecisionTreeRegressor

d_reg = DecisionTreeRegressor()
d_reg.fit(X_train, y_train)

d_importance = d_reg.feature_importances_
for i,v in enumerate(d_importance):
	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names[i],v))
 
# plot feature importance
pyplot.bar([x for x in range(len(d_importance))], d_importance)
pyplot.show()

print('Accuracy of Decision Tree regressor on training set: {:.2f}'
     .format(d_reg.score(X_train, y_train)))
print('Accuracy of Decision Tree regressor on test set: {:.2f}'
     .format(d_reg.score(X_test, y_test)))

"""**Random Forest Regression Feature Importance**"""

from sklearn.ensemble import RandomForestRegressor

rf_reg = RandomForestRegressor()
rf_reg.fit(X_train, y_train)

rf_importance = rf_reg.feature_importances_
for i,v in enumerate(rf_importance):
	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names[i],v))
 
# plot feature importance
pyplot.bar([x for x in range(len(rf_importance))], rf_importance)
pyplot.show()

print('Accuracy of Random Forest regressor on training set: {:.2f}'
     .format(rf_reg.score(X_train, y_train)))
print('Accuracy of Random Forest regressor on test set: {:.2f}'
     .format(rf_reg.score(X_test, y_test)))

"""**XGBoost Regression Feature Importance**"""

from xgboost import XGBRegressor

xgb_reg = XGBRegressor()
xgb_reg.fit(X_train, y_train)

xgb_importance = xgb_reg.feature_importances_
for i,v in enumerate(xgb_importance):
	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names[i],v))
 
# plot feature importance
pyplot.bar([x for x in range(len(xgb_importance))], xgb_importance)
pyplot.show()

print('Accuracy of Xgboost regressor on training set: {:.2f}'
     .format(xgb_reg.score(X_train, y_train)))
print('Accuracy of Xgboost regressor on test set: {:.2f}'
     .format(xgb_reg.score(X_test, y_test)))

"""**Permutation Feature Importance for Regression**"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.inspection import permutation_importance

knn_reg = KNeighborsRegressor()
knn_reg.fit(X_train, y_train)

results = permutation_importance(knn_reg, X_train, y_train, scoring='neg_mean_squared_error')
knn_importance = results.importances_mean

for i,v in enumerate(knn_importance):
	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names[i],v))
 
# plot feature importance
pyplot.bar([x for x in range(len(knn_importance))], knn_importance)
pyplot.show()

print('Accuracy of KNN regressor on training set: {:.2f}'
     .format(knn_reg.score(X_train, y_train)))
print('Accuracy of KNN regressor on test set: {:.2f}'
     .format(knn_reg.score(X_test, y_test)))

Test_names = ['Linear regression', 'Randomforest', 'CART', 'Xgboost',
              'k-neighbors']
name_code = ['LR', 'RF', 'CART', 'xG', 'k-N']

training = []
test = []

training.append(linear_reg.score(X_train, y_train))
training.append(rf_reg.score(X_train, y_train)) 
training.append(d_reg.score(X_train, y_train))
training.append(xgb_reg.score(X_train, y_train))
training.append(knn_reg.score(X_train, y_train))

test.append(linear_reg.score(X_test, y_test))
test.append(rf_reg.score(X_test, y_test)) 
test.append(d_reg.score(X_test, y_test))
test.append(xgb_reg.score(X_test, y_test))
test.append(knn_reg.score(X_test, y_test))

plt.scatter(name_code,training,label='training')
plt.scatter(name_code,test,label='test')
plt.xlabel('Name of the test')
plt.ylabel('Accuracy for each test')
plt.legend()
plt.show()

#Random Forest and XGboost have the same highest test accuracy but XGboost importance give infinite priorities
#Therefore Random Forest is selected

A = np.ones([len(feature_names),len(feature_names)])
for j,p in enumerate(rf_importance):
  for i,v in enumerate(rf_importance):
    A[j,i] = float("{:.3f}".format(p/v))
    print('Importance of %s over %s is  Score: %.3f' % (feature_names[j],feature_names[i],p/v))

ar = np.array(A)
print(ar)

priority = []
for row in A:
 v = 1
 for i in row:
   v = v * i
 v = v ** (1/len(feature_names))
 priority.append(v)

#Priority
for i in range(len(feature_names)):
  print(feature_names[i],priority[i])

pr = np.array(priority)
p = sum(pr)
print(p)

weights = []
for i in priority:
  value = float("{:.4f}".format(i/p))
  weights.append(value)

for i in range(len(feature_names)):
  print(feature_names[i],weights[i])

thisdict = {}
for i in range(len(feature_names)):
  thisdict[feature_names[i]] = weights[i]
print(thisdict)

df['Bowler_score_CareerFeatureEcon'] = df['Height (cm)']*thisdict['Height (cm)']+df['Man of the match']*thisdict['Man of the match']+df['Bowling Style']*thisdict['Bowling Style']+df['Runs']*thisdict['Runs']+df['Wkts']*thisdict['Wkts']+df['Ave']*thisdict['Ave']+df['Match_Win_Ave']*thisdict['Match_Win_Ave']+df['SR']*thisdict['SR']+df['4']*thisdict['4']+df['5']*thisdict['5']+df['Hat tricks']*thisdict['Hat tricks']+df['BBI-calc']*thisdict['BBI-calc']

df.head()

df_new = df[['Player', 'Bowler_score_CareerFeatureEcon']]

df_new.head(20)

df_new.sort_values(by=['Bowler_score_CareerFeatureEcon'], ascending=False)

from google.colab import files
df_new.to_csv('careerFeatureEcon_bowler_score.csv' , index=False)
files.download('careerFeatureEcon_bowler_score.csv')

"""## **With All Features**"""

feature_names = ['Height (cm)', 'Man of the match', 'Bowling Style', 'Runs', 'Wkts', 'Ave', 'Match_Win_Ave', 'SR', '4', '5', 'Hat tricks', 'BBI-calc','Mat','Inns']
X = df[feature_names]
Y = df["Econ"]

#split the data set as training set and test set randomly
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)

#apply scaling
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""**Linear Regression Feature importance**"""

from sklearn.linear_model import LinearRegression
from matplotlib import pyplot

linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)

lin_importance = linear_reg.coef_
for i,v in enumerate(lin_importance):
	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names[i],v))
 
# plot feature importance
pyplot.bar([x for x in range(len(lin_importance))], lin_importance)
pyplot.show()

print('Accuracy of Linear regressor on training set: {:.2f}'
     .format(linear_reg.score(X_train, y_train)))
print('Accuracy of Linear regressor on test set: {:.2f}'
     .format(linear_reg.score(X_test, y_test)))

"""**CART Regression Feature Importance**"""

from sklearn.tree import DecisionTreeRegressor

d_reg = DecisionTreeRegressor()
d_reg.fit(X_train, y_train)

d_importance = d_reg.feature_importances_
for i,v in enumerate(d_importance):
	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names[i],v))
 
# plot feature importance
pyplot.bar([x for x in range(len(d_importance))], d_importance)
pyplot.show()

print('Accuracy of Decision Tree regressor on training set: {:.2f}'
     .format(d_reg.score(X_train, y_train)))
print('Accuracy of Decision Tree regressor on test set: {:.2f}'
     .format(d_reg.score(X_test, y_test)))

"""**Random Forest Regression Feature Importance**"""

from sklearn.ensemble import RandomForestRegressor

rf_reg = RandomForestRegressor()
rf_reg.fit(X_train, y_train)

rf_importance = rf_reg.feature_importances_
for i,v in enumerate(rf_importance):
	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names[i],v))
 
# plot feature importance
pyplot.bar([x for x in range(len(rf_importance))], rf_importance)
pyplot.show()

print('Accuracy of Random Forest regressor on training set: {:.2f}'
     .format(rf_reg.score(X_train, y_train)))
print('Accuracy of Random Forest regressor on test set: {:.2f}'
     .format(rf_reg.score(X_test, y_test)))

"""**XGBoost Regression Feature Importance**"""

from xgboost import XGBRegressor

xgb_reg = XGBRegressor()
xgb_reg.fit(X_train, y_train)

xgb_importance = xgb_reg.feature_importances_
for i,v in enumerate(xgb_importance):
	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names[i],v))
 
# plot feature importance
pyplot.bar([x for x in range(len(xgb_importance))], xgb_importance)
pyplot.show()

print('Accuracy of Xgboost regressor on training set: {:.2f}'
     .format(xgb_reg.score(X_train, y_train)))
print('Accuracy of Xgboost regressor on test set: {:.2f}'
     .format(xgb_reg.score(X_test, y_test)))

"""**Permutation Feature Importance for Regression**"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.inspection import permutation_importance

knn_reg = KNeighborsRegressor()
knn_reg.fit(X_train, y_train)

results = permutation_importance(knn_reg, X_train, y_train, scoring='neg_mean_squared_error')
knn_importance = results.importances_mean

for i,v in enumerate(knn_importance):
	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names[i],v))
 
# plot feature importance
pyplot.bar([x for x in range(len(knn_importance))], knn_importance)
pyplot.show()

print('Accuracy of KNN regressor on training set: {:.2f}'
     .format(knn_reg.score(X_train, y_train)))
print('Accuracy of KNN regressor on test set: {:.2f}'
     .format(knn_reg.score(X_test, y_test)))

Test_names = ['Linear regression', 'Randomforest', 'CART', 'Xgboost',
              'k-neighbors']
name_code = ['LR', 'RF', 'CART', 'xG', 'k-N']

training = []
test = []

training.append(linear_reg.score(X_train, y_train))
training.append(rf_reg.score(X_train, y_train)) 
training.append(d_reg.score(X_train, y_train))
training.append(xgb_reg.score(X_train, y_train))
training.append(knn_reg.score(X_train, y_train))

test.append(linear_reg.score(X_test, y_test))
test.append(rf_reg.score(X_test, y_test)) 
test.append(d_reg.score(X_test, y_test))
test.append(xgb_reg.score(X_test, y_test))
test.append(knn_reg.score(X_test, y_test))

plt.scatter(name_code,training,label='training')
plt.scatter(name_code,test,label='test')
plt.xlabel('Name of the test')
plt.ylabel('Accuracy for each test')
plt.legend()
plt.show()

#Random Forest and XGboost have the same highest test accuracy but XGboost importance give infinite priorities
#Therefore Random Forest is selected

A = np.ones([len(feature_names),len(feature_names)])
for j,p in enumerate(rf_importance):
  for i,v in enumerate(rf_importance):
    A[j,i] = float("{:.3f}".format(p/v))
    print('Importance of %s over %s is  Score: %.3f' % (feature_names[j],feature_names[i],p/v))

ar = np.array(A)
print(ar)

priority = []
for row in A:
 v = 1
 for i in row:
   v = v * i
 v = v ** (1/len(feature_names))
 priority.append(v)

#Priority
for i in range(len(feature_names)):
  print(feature_names[i],priority[i])

pr = np.array(priority)
p = sum(pr)
print(p)

weights = []
for i in priority:
  value = float("{:.4f}".format(i/p))
  weights.append(value)

for i in range(len(feature_names)):
  print(feature_names[i],weights[i])

thisdict = {}
for i in range(len(feature_names)):
  thisdict[feature_names[i]] = weights[i]
print(thisdict)

df['Bowler_score_AllFeatureEcon'] = df['Height (cm)']*thisdict['Height (cm)']+df['Man of the match']*thisdict['Man of the match']+df['Bowling Style']*thisdict['Bowling Style']+df['Runs']*thisdict['Runs']+df['Wkts']*thisdict['Wkts']+df['Ave']*thisdict['Ave']+df['Match_Win_Ave']*thisdict['Match_Win_Ave']+df['SR']*thisdict['SR']+df['4']*thisdict['4']+df['5']*thisdict['5']+df['Hat tricks']*thisdict['Hat tricks']+df['BBI-calc']*thisdict['BBI-calc']+df['Mat']*thisdict['Mat']+df['Inns']*thisdict['Inns']

df.head()

df_new = df[['Player', 'Bowler_score_AllFeatureEcon']]

df_new.head(20)

df_new.sort_values(by=['Bowler_score_AllFeatureEcon'], ascending=False)

from google.colab import files
df_new.to_csv('AllFeatureEcon_bowler_score.csv' , index=False)
files.download('AllFeatureEcon_bowler_score.csv')