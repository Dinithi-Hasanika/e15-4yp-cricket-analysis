# -*- coding: utf-8 -*-
"""CO421-Bowling-comparison.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FkU3yyI39qr4ntEuNbGPnH1kZTy1PCkH
"""

from google.colab import auth
auth.authenticate_user()
import gspread
from oauth2client.client import GoogleCredentials
gc = gspread.authorize(GoogleCredentials.get_application_default())

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

wb = gc.open_by_url('https://docs.google.com/spreadsheets/d/1peLpNFoEu8NHtVPttJXz4-aScixYu5dh_-6gYPOi9rQ/edit?ts=5f7f50be#gid=1402420443')

sheet = wb.worksheet('bo_data_clean')

data = sheet.get_all_values()

df = pd.DataFrame(data)
df.columns = df.iloc[0]
df = df.iloc[1:]

df.head(43)

del df["u"]
del df["v"]
del df["w"]
del df["Bowler Score"]
del df["BBI"]
del df["BBI-copy-1"]
del df["BBI-copy-2"]
del df["Span"]

df.dtypes

df.__eq__('').sum()

df.__eq__('-').sum()

# df.replace('',"?",inplace = True)
# df.replace('-',"?",inplace = True)

# df.__eq__('?').sum()

# df.replace('?',np.nan,inplace = True)

# df.__eq__('?').sum()

df["Height (cm)"]= df["Height (cm)"].astype(float)

print(df.info())

print(df['Height (cm)'].unique())

print(df['Bowling Style'].unique())

from statistics import mean
df["Height (cm)"].fillna(df["Height (cm)"].mean(),inplace = True)

df.head()

import statistics
from statistics import mode
print(statistics.mode(df['Bowling Style']))

df['Bowling Style'].fillna('Right-arm fast-medium',inplace=True)

print(df.isnull().sum())

df["Bowling Style"]= df["Bowling Style"].astype('category')
df["Bowling Style"]= df["Bowling Style"].cat.codes

df["Mat"]= df["Mat"].astype(int)
df["Inns"]= df["Inns"].astype(int)
df["Balls"]= df["Balls"].astype(int)
df["Runs"]= df["Runs"].astype(int)
df["Wkts"]= df["Wkts"].astype(int)
df["BBI-calc"]= df["BBI-calc"].astype(float)
df["SR"]= df["SR"].astype(float)
df["Ave"]= df["Ave"].astype(float)
df["Econ"]= df["Econ"].astype(float)
df["4"]= df["4"].astype(int)
df["Man of the match"]= df["Man of the match"].astype(int)
df["5"]= df["5"].astype(int)
df["Hat tricks"]= df["Hat tricks"].astype(int)

print(df.info())

"""# **Wkts Vs Other features**"""

feature_names = ['Man of the match','Height (cm)','Bowling Style','Mat','Inns','Balls','Runs','BBI-calc','Ave','SR','Econ','4','5','Hat tricks']
X = df[feature_names]
Y = df['Wkts']

#split the data set as training set and test set randomly
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)

#apply scaling
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from matplotlib import pyplot

"""***1.Xgboost***"""

from xgboost import XGBRegressor
# define the model
model_xg = XGBRegressor()
# fit the model
model_xg.fit(X_train, y_train)
# get importance
importance_xg = model_xg.feature_importances_
# summarize feature importance
for i,v in enumerate(importance_xg):
	print('Feature: %0d, -> %s Score: %.5f' % (i,feature_names[i] ,v))
# plot feature importance
plt.bar([x for x in range(len(importance_xg))], importance_xg)
pyplot.xticks(np.arange(len(feature_names)), feature_names,rotation='vertical')
plt.show()

print('Accuracy of Xgboost regressor on training set: {:.2f}'
     .format(model_xg.score(X_train, y_train)))
print('Accuracy of Xgboost regressor on test set: {:.2f}'
     .format(model_xg.score(X_test, y_test)))

"""***2.Permutation***"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.inspection import permutation_importance

knn_reg = KNeighborsRegressor()
knn_reg.fit(X_train, y_train)
# perform permutation importance
resultsk = permutation_importance(knn_reg, X_train, y_train, scoring='neg_mean_squared_error')

perm_importance = resultsk.importances_mean
for i,v in enumerate(perm_importance):
	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names[i],v))
 
# plot feature importance
pyplot.bar([x for x in range(len(perm_importance))], perm_importance)
pyplot.xticks(np.arange(len(feature_names)), feature_names,rotation='vertical')
pyplot.show()

print('Accuracy of knn regressor on training set: {:.2f}'
     .format(knn_reg.score(X_train, y_train)))
print('Accuracy of knn regressor on test set: {:.2f}'
     .format(knn_reg.score(X_test, y_test)))

"""***3.Linear Regression***"""

from sklearn.linear_model import LinearRegression
model_reg = LinearRegression()
model_reg.fit(X_train, y_train)
importance1 = model_reg.coef_
for i,v in enumerate(importance1):
	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names[i],v))
pyplot.bar([x for x in range(len(importance1))], importance1)
pyplot.xticks(np.arange(len(feature_names)), feature_names,rotation='vertical')
pyplot.show()

print('Accuracy of Linear regression classifier on training set: {:.2f}'
     .format(model_reg.score(X_train, y_train)))
print('Accuracy of Linear regression classifier on test set: {:.2f}'
     .format(model_reg.score(X_test, y_test)))

"""***4.CART Regression Feature Importance***"""

from sklearn.tree import DecisionTreeRegressor
model_dt = DecisionTreeRegressor()
model_dt.fit(X_train, y_train)
importance_dt = model_dt.feature_importances_
# summarize feature importance
for i,v in enumerate(importance_dt):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
pyplot.bar([x for x in range(len(importance_dt))], importance_dt)
pyplot.xticks(np.arange(len(feature_names)), feature_names,rotation='vertical')
pyplot.show()

print('Accuracy of DecisionTree Regressor on training set: {:.2f}'
     .format(model_dt.score(X_train, y_train)))
print('Accuracy of DecisionTree Regressor on test set: {:.2f}'
     .format(model_dt.score(X_test, y_test)))

"""***5.Random Forest***"""

from sklearn.ensemble import RandomForestClassifier
modelrf = RandomForestClassifier()
modelrf.fit(X_train, y_train)
importance3 = modelrf.feature_importances_
for i,v in enumerate(importance3):
	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names[i],v))
pyplot.bar([x for x in range(len(importance3))], importance3)
pyplot.xticks(np.arange(len(feature_names)), feature_names,rotation='vertical')
pyplot.show()

print('Accuracy of Random forest classifier on training set: {:.2f}'
     .format(modelrf.score(X_train, y_train)))
print('Accuracy of Random forest classifier on test set: {:.2f}'
     .format(modelrf.score(X_test, y_test)))

# from sklearn.svm import SVR
# regressor = SVR(kernel = 'linear')
# regressor.fit(X_train, y_train)
# importance_svm = regressor.coef_
# for i,v in enumerate(importance_svm):
# 	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names[i],v))
# pyplot.bar([x for x in range(len(importance_svm))], importance_svm)
# pyplot.xticks(np.arange(len(feature_names)), feature_names,rotation='vertical')
# pyplot.show()

Test_names = ['Linear regression', 'Decision Tree','k-neighbors','Randomforest','XGBoot']
Test_name_repres = ['LR', 'DT','k-NN', 'RF','XGB']

training = []
test = []

training.append(model_reg.score(X_train, y_train))
training.append(model_dt.score(X_train, y_train))
training.append(knn_reg.score(X_train, y_train))
training.append(modelrf.score(X_train, y_train))
training.append(model_xg.score(X_train, y_train))

test.append(model_reg.score(X_test, y_test))
test.append(model_dt.score(X_test, y_test))
test.append(knn_reg.score(X_test, y_test))
test.append(modelrf.score(X_test, y_test))
test.append(model_xg.score(X_test, y_test))

plt.scatter(Test_name_repres,training,label='training')
plt.scatter(Test_name_repres,test,label='test')
plt.xlabel('Name of the test')
plt.ylabel('Accuracy for each test')
plt.legend()
plt.show()

"""# **Hat Tricks Vs other features** 
(this combination seems no use)
"""

feature_names1 = ['Man of the match','Height (cm)','Bowling Style','Mat','Inns','Balls','Runs','BBI-calc','Ave','SR','Econ','4','5','Wkts']
X1 = df[feature_names]
Y1 = df['Hat tricks']

#split the data set as training set and test set randomly
from sklearn.model_selection import train_test_split
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, Y1, random_state=0)

#apply scaling
from sklearn.preprocessing import MinMaxScaler
scaler1 = MinMaxScaler()
X_train1 = scaler1.fit_transform(X_train1)
X_test1 = scaler1.transform(X_test1)

from matplotlib import pyplot

"""***1.XGBoost***"""

from xgboost import XGBRegressor
# define the model
model_xg1 = XGBRegressor()
# fit the model
model_xg1.fit(X_train1, y_train1)
# get importance
importance_xg1 = model_xg1.feature_importances_
# summarize feature importance
for i,v in enumerate(importance_xg1):
	print('Feature: %0d, -> %s Score: %.5f' % (i,feature_names1[i] ,v))
# plot feature importance
plt.bar([x for x in range(len(importance_xg1))], importance_xg1)
pyplot.xticks(np.arange(len(feature_names1)), feature_names1,rotation='vertical')
plt.show()

"""***2.Permutation***"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.inspection import permutation_importance

knn_reg1 = KNeighborsRegressor()
knn_reg1.fit(X_train1, y_train1)
# perform permutation importance
resultsk1 = permutation_importance(knn_reg1, X_train1, y_train1, scoring='neg_mean_squared_error')

perm_importance1 = resultsk1.importances_mean
for i,v in enumerate(perm_importance1):
	print('Feature:%0d -> %s, Score: %.5f' % (i,feature_names1[i],v))
 
# plot feature importance
pyplot.bar([x for x in range(len(perm_importance1))], perm_importance1)
pyplot.xticks(np.arange(len(feature_names1)), feature_names1,rotation='vertical')
pyplot.show()